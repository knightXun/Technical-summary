Top
NSD CLUSTER DAY03

案例1：实验环境
案例2：部署ceph集群
案例3：创建Ceph块存储
1 案例1：实验环境
1.1 问题

准备四台KVM虚拟机，其三台作为存储集群节点，一台安装为客户端，实现如下功能：
创建1台客户端虚拟机
创建3台存储集群虚拟机
配置主机名、IP地址、YUM源
修改所有主机的主机名
配置无密码SSH连接
配置NTP时间同步
创建虚拟机磁盘
1.2 方案

使用4台虚拟机，1台客户端、3台存储集群服务器，拓扑结构如图-1所示。

图-1
所有主机的主机名及对应的IP地址如表-1所示。
注意：所有主机基本系统光盘的YUM源必须提前配置好。
表－1 主机名称及对应IP地址表

1.3 步骤

实现此案例需要按照如下步骤进行。
步骤一：安装前准备

1）物理机为所有节点配置yum源服务器。
提示：ceph10.iso在/linux-soft/02目录。
[root@room9pc01 ~]# mkdir  /var/ftp/ceph
[root@room9pc01 ~]# mount ceph10.iso /var/ftp/ceph/
2）配置无密码连接(包括自己远程自己也不需要密码)，在node1操作。
[root@node1 ~]# ssh-keygen   -f /root/.ssh/id_rsa    -N ''
[root@node1 ~]# for i in 10  11  12  13
 do
     ssh-copy-id  192.168.4.$i
 done
3）修改/etc/hosts并同步到所有主机。
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
 [root@node1 ~]# cat /etc/hosts
... ...
192.168.4.10  client
192.168.4.11     node1
192.168.4.12     node2
192.168.4.13     node3
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
[root@node1 ~]# for i in client node1  node2  node3
do
scp  /etc/hosts   $i:/etc/
done
4）修改所有节点都需要配置YUM源，并同步到所有主机。
[root@node1 ~]# cat /etc/yum.repos.d/ceph.repo
[mon]
name=mon
baseurl=ftp://192.168.4.254/ceph/MON
gpgcheck=0
[osd]
name=osd
baseurl=ftp://192.168.4.254/ceph/OSD
gpgcheck=0
[tools]
name=tools
baseurl=ftp://192.168.4.254/ceph/Tools
gpgcheck=0
[root@node1 ~]# yum repolist                #验证YUM源软件数量
源标识            源名称                    状态
Dvd                redhat                    9,911
Mon                mon                        41
Osd                osd                        28
Tools            tools                    33
repolist: 10,013
[root@node1 ~]# for i in  client  node1  node2  node3
do
scp  /etc/yum.repos.d/ceph.repo   $i:/etc/yum.repos.d/
done
5）所有节点主机与真实主机的NTP服务器同步时间。
提示：默认真实物理机已经配置为NTP服务器。
[root@node1 ~]# vim /etc/chrony.conf
… …
server 192.168.4.254   iburst
[root@node1 ~]# for i in client  node1  node2  node3
do
     scp /etc/chrony.conf $i:/etc/
     ssh  $i  "systemctl restart chronyd"
done
步骤三：准备存储磁盘

物理机上为每个虚拟机准备3块20G磁盘（可以使用命令，也可以使用图形直接添加）。
 [root@room9pc01 ~]# virt-manager
2 案例2：部署ceph集群
2.1 问题

沿用练习一，部署Ceph集群服务器，实现以下目标：
安装部署工具ceph-deploy
创建ceph集群
准备日志磁盘分区
创建OSD存储空间
查看ceph状态，验证
2.2 步骤

实现此案例需要按照如下步骤进行。
步骤一：安装部署软件ceph-deploy

1）在node1安装部署工具，学习工具的语法格式。
[root@node1 ~]#  yum -y install ceph-deploy
[root@node1 ~]#  ceph-deploy  --help
[root@node1 ~]#  ceph-deploy mon --help
2）创建目录
[root@node1 ~]#  mkdir ceph-cluster
[root@node1 ~]#  cd ceph-cluster/
步骤二：部署Ceph集群

1）创建Ceph集群配置,在ceph-cluster目录下生成Ceph配置文件。
在ceph.conf配置文件中定义monitor主机是谁。
[root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3
2）给所有节点安装ceph相关软件包。
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
    ssh  $i "yum -y install ceph-mon ceph-osd ceph-mds ceph-radosgw"
done 
3）初始化所有节点的mon服务，也就是启动mon服务（主机名解析必须对）。
[root@node1 ceph-cluster]# ceph-deploy mon create-initial
常见错误及解决方法（非必要操作，有错误可以参考）：
如果提示如下错误信息：
[node1][ERROR ] admin_socket: exception getting command descriptions: [Error 2] No such file or directory
解决方案如下（在node1操作）：
先检查自己的命令是否是在ceph-cluster目录下执行的！！！！如果确认是在该目录下执行的create-initial命令，依然报错，可以使用如下方式修复。
[root@node1 ceph-cluster]# vim ceph.conf      #文件最后追加以下内容
public_network = 192.168.4.0/24
修改后重新推送配置文件:
[root@node1 ceph-cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3
步骤三：创建OSD

备注：vdb1和vdb2这两个分区用来做存储服务器的journal缓存盘。
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
     ssh $i "parted /dev/vdb mklabel gpt"
     ssh $i "parted /dev/vdb mkpart primary 1 50%"
     ssh $i "parted /dev/vdb mkpart primary 50% 100%"
 done
2）磁盘分区后的默认权限无法让ceph软件对其进行读写操作，需要修改权限。
node1、node2、node3都需要操作，这里以node1为例。
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb1
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb2
#上面的权限修改为临时操作，重启计算机后，权限会再次被重置。
#我们还需要将规则写到配置文件实现永久有效。
#规则：如果设备名称为/dev/vdb1则设备文件的所有者和所属组都设置为ceph。
#规则：如果设备名称为/dev/vdb2则设备文件的所有者和所属组都设置为ceph。
[root@node1 ceph-cluster]# vim /etc/udev/rules.d/70-vdb.rules
ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
3）初始化清空磁盘数据（仅node1操作即可）。
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:vdc   node1:vdd    
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:vdc   node2:vdd
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:vdc   node3:vdd   
4）创建OSD存储空间（仅node1操作即可）
重要：很多同学在这里会出错！将主机名、设备名称输入错误！！！
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2  
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
//每个存储设备对应一个缓存设备，缓存需要SSD，不需要很大
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2 
常见错误及解决方法（非必须操作）。
使用osd create创建OSD存储空间时，如提示下面的错误提示：
[ceph_deploy][ERROR ] RuntimeError: bootstrap-osd keyring not found; run 'gatherkeys'
可以使用如下命令修复文件，重新配置ceph的密钥文件：
[root@node1 ceph-cluster]#  ceph-deploy gatherkeys node1 node2 node3 
步骤四：验证测试

1) 查看集群状态。
[root@node1 ~]#  ceph  -s
2）常见错误（非必须操作）。
如果查看状态包含如下信息：
health: HEALTH_WARN
        clock skew detected on  node2, node3…  
clock skew表示时间不同步，解决办法：请先将所有主机的时间都使用NTP时间同步！！！
Ceph要求所有主机时差不能超过0.05s，否则就会提示WARN。
如果状态还是失败，可以尝试执行如下命令，重启ceph服务：
[root@node1 ~]#  systemctl restart ceph\*.service ceph\*.target
3 案例3：创建Ceph块存储
3.1 问题

沿用练习一，使用Ceph集群的块存储功能，实现以下目标：
创建块存储镜像
客户端映射镜像
创建镜像快照
使用快照还原数据
使用快照克隆镜像
删除快照与镜像
3.2 步骤

实现此案例需要按照如下步骤进行。
步骤一：创建镜像

1）查看存储池。
[root@node1 ~]# ceph osd lspools
0 rbd,
2）创建镜像、查看镜像
[root@node1 ~]# rbd create demo-image --image-feature  layering --size 10G
[root@node1 ~]# rbd create rbd/image  --image-feature  layering --size 10G
#这里的demo-image和image为创建的镜像名称，可以为任意字符。
#--image-feature参数指定我们创建的镜像有哪些功能，layering是开启COW功能。
#提示：ceph镜像支持很多功能，但很多是操作系统不支持的，我们只开启layering。
[root@node1 ~]# rbd list
[root@node1 ~]# rbd info demo-image
rbd image 'demo-image':
    size 10240 MB in 2560 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.d3aa2ae8944a
    format: 2
    features: layering
步骤二：动态调整

1）缩小容量
[root@node1 ~]# rbd resize --size 7G image --allow-shrink
[root@node1 ~]# rbd info image
2）扩容容量
[root@node1 ~]# rbd resize --size 15G image
[root@node1 ~]# rbd info image
步骤三：通过KRBD访问

1）客户端通过KRBD访问
#客户端需要安装ceph-common软件包
#拷贝配置文件（否则不知道集群在哪）
#拷贝连接密钥（否则无连接权限）
[root@client ~]# yum -y  install ceph-common
[root@client ~]# scp 192.168.4.11:/etc/ceph/ceph.conf  /etc/ceph/
[root@client ~]# scp 192.168.4.11:/etc/ceph/ceph.client.admin.keyring \
/etc/ceph/
[root@client ~]# rbd map image
[root@client ~]#  lsblk
[root@client ~]# rbd showmapped
id pool image snap device    
0  rbd  image -    /dev/rbd0
2) 客户端格式化、挂载分区
[root@client ~]# mkfs.xfs /dev/rbd0
[root@client ~]# mount /dev/rbd0 /mnt/
[root@client ~]# echo "test" > /mnt/test.txt
步骤四：创建镜像快照

1) 查看镜像快照（默认所有镜像都没有快照）。
 [root@node1 ~]# rbd snap ls image
2) 给镜像创建快照。
[root@node1 ~]# rbd snap create image --snap image-snap1
#为image镜像创建快照，快照名称为image-snap1
[root@node1 ~]# rbd snap ls image
SNAPID NAME            SIZE 
     4 image-snap1 15360 MB
3) 删除客户端写入的测试文件
[root@client ~]# rm  -rf   /mnt/test.txt
[root@client ~]# umount  /mnt
4) 还原快照
[root@node1 ~]# rbd snap rollback image --snap image-snap1
#客户端重新挂载分区
[root@client ~]# mount /dev/rbd0 /mnt/
[root@client ~]# ls  /mnt
步骤四：创建快照克隆

1）克隆快照
[root@node1 ~]#  rbd snap protect image --snap image-snap1
[root@node1 ~]#  rbd snap rm image --snap image-snap1    //会失败
[root@node1 ~]#  rbd clone \
image --snap image-snap1 image-clone --image-feature layering
//使用image的快照image-snap1克隆一个新的名称为image-clone镜像
2）查看克隆镜像与父镜像快照的关系
[root@node1 ~]#  rbd info image-clone
rbd image 'image-clone':
    size 15360 MB in 3840 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.d3f53d1b58ba
    format: 2
    features: layering
    flags: 
    parent: rbd/image@image-snap1
#克隆镜像很多数据都来自于快照链
#如果希望克隆镜像可以独立工作，就需要将父快照中的数据，全部拷贝一份，但比较耗时！！！
[root@node1 ~]#  rbd flatten image-clone
[root@node1 ~]#  rbd info image-clone
rbd image 'image-clone':
    size 15360 MB in 3840 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.d3f53d1b58ba
    format: 2
    features: layering
    flags: 
#注意，父快照信息没了！
[root@node1 ~]#  rbd snap unprotect image --snap image-snap1     #取消快照保护
[root@node1 ~]#  rbd snap rm image --snap image-snap1            #可以删除快照
步骤四：其他操作

1） 客户端撤销磁盘映射
[root@client ~]# umount /mnt
[root@client ~]# rbd showmapped
id pool image        snap device    
0  rbd  image        -    /dev/rbd0
//语法格式:
[root@client ~]# rbd unmap /dev/rbd0





案例1：Keepalived高可用
案例2：部署Ceph分布式存储
1 案例1：Keepalived高可用
1.1 问题

部署两台代理服务器，实现如下效果：
利用keepalived实现两台代理服务器的高可用
配置VIP为192.168.4.80
修改对应的域名解析记录
1.2 方案

实验拓扑如图-1所示，做具体实验前请先配置好环境。

图-1
备注：实际操作中DNS服务代理服务器部署在同一台主机上（节约虚拟机资源）。
主机配置如表-1所示。
表-1

1.3 步骤

实现此案例需要按照如下步骤进行。
步骤一：配置第二台代理服务器

1）部署HAProxy
安装软件，手动修改配置文件，添加如下内容。
[root@proxy2 ~]# yum -y install haproxy 
[root@proxy2 ~]# vim /etc/haproxy/haproxy.cfg
listen wordpress *:80
  balance roundrobin
  server web1 192.168.2.11:80 check inter 2000 rise 2 fall 3
  server web2 192.168.2.12:80 check inter 2000 rise 2 fall 3
  server web3 192.168.2.13:80 check inter 2000 rise 2 fall 3
[root@proxy2 ~]# systemctl start haproxy
[root@proxy2 ~]# systemctl enable haproxy
步骤二：为两台代理服务器配置keepalived

1）配置第一台代理服务器proxy（192.168.4.5）。
[root@proxy ~]# yum install -y keepalived
[root@proxy ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  router_id  proxy1                        //设置路由ID号
  vrrp_iptables                               //不添加任何防火墙规则
}
vrrp_instance VI_1 {
  state MASTER                         //主服务器为MASTER（备服务器需要修改为BACKUP）
  interface eth0                    //定义网络接口
  virtual_router_id 51                
  priority 100                     //服务器优先级,优先级高优先获取VIP（实验需要修改）
  advert_int 1
  authentication {
    auth_type pass
    auth_pass 1111                       //主备服务器密码必须一致
  }
  virtual_ipaddress {                   //谁是主服务器谁获得该VIP（实验需要修改）
192.168.4.80 
}    
}
[root@proxy ~]# systemctl start keepalived
！！！重要！！！
在全局配置global_defs{}中手动添加vrrp_iptables，即可解决防火墙的问题。
2）配置第二台代理服务器proxy（192.168.4.6）。
[root@proxy2 ~]# yum install -y keepalived
[root@proxy2 ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  router_id  proxy2                        //设置路由ID号
vrrp_iptables                               //不添加任何防火墙规则
}
vrrp_instance VI_1 {
  state BACKUP                         //主服务器为MASTER（备服务器需要修改为BACKUP）
  interface eth0                    //定义网络接口
  virtual_router_id 51                
  priority 50                         //服务器优先级,优先级高优先获取VIP
  advert_int 1
  authentication {
    auth_type pass
    auth_pass 1111                       //主备服务器密码必须一致
  }
  virtual_ipaddress {                   //谁是主服务器谁获得该VIP
192.168.4.80 
}    
}
[root@proxy2 ~]# systemctl start keepalived
！！！重要！！！
在全局配置global_defs{}中手动添加vrrp_iptables，即可解决防火墙的问题。
步骤三：修改DNS服务器

1）修改网站域名对应的解析记录，解析到新的VIP地址。
192.168.4.5为DNS服务器。
[root@proxy ~]# vim /var/named/lab.com.zone
$TTL 1D
@       IN SOA  @ rname.invalid. (
                                        0       ; serial
                                        1D      ; refresh
                                        1H      ; retry
                                        1W      ; expire
                                        3H )    ; minimum
@       NS      dns.lab.com.
dns     A       192.168.4.5
www     A       192.168.4.80
2）重启DNS服务
[root@proxy ~]# systemctl restart named
2 案例2：部署Ceph分布式存储
2.1 问题

部署Ceph分布式存储，实现如下效果：
使用三台服务器部署Ceph分布式存储
实现Ceph文件系统共享
将网站数据从NFS迁移到Ceph存储
2.2 方案

实验拓扑如图-2所示，做具体实验前请先配置好环境。

图-2
备注：实际操作中DNS服务代理服务器部署在同一台主机上（节约虚拟机资源）。
主机配置如表-2所示。
表-2

2.3 步骤

实现此案例需要按照如下步骤进行。
步骤一：准备实验环境

1）物理机为所有节点配置yum源服务器。
提示：ceph10.iso在/linux-soft/02目录。
[root@room9pc01 ~]# mkdir  /var/ftp/ceph
[root@room9pc01 ~]# mount ceph10.iso /var/ftp/ceph/
2）在node1配置SSH密钥，让node1可用无密码连接node1,node2,node3
[root@node1 ~]# ssh-keygen  -f /root/.ssh/id_rsa  -N  ''
[root@node1 ~]# for i in   41  42  43
do
ssh-copy-id  192.168.2.$i
done
3)修改/etc/hosts域名解析记录（不要删除原有的数据），同步给所有ceph节点。
[root@node1 ~]# vim /etc/hosts
192.168.2.41    node1
192.168.2.42     node2
192.168.2.43    node3
[root@node1 ~]# for i in 41 42 43
do
     scp /etc/hosts 192.168.2.$i:/etc
done
4）为所有ceph节点配置yum源，并将配置同步给所有节点
[root@node1 ~]# cat /etc/yum.repos.d/ceph.repo
[mon]
name=mon
baseurl=ftp://192.168.2.254/ceph/MON
gpgcheck=0
[osd]
name=osd
baseurl=ftp://192.168.2.254/ceph/OSD
gpgcheck=0
[tools]
name=tools
baseurl=ftp://192.168.2.254/ceph/Tools
gpgcheck=0
[root@node1 ~]# yum repolist                #验证YUM源软件数量
源标识            源名称                    状态
Dvd                redhat                    9,911
Mon                mon                        41
Osd                osd                        28
Tools            tools                    33
repolist: 10,013
[root@node1 ~]# for i in 41 42 43
do
     scp /etc/yum.repos.d/ceph.repo 192.168.2.$i:/etc/yum.repos.d/
done
5）所有节点主机与真实主机的NTP服务器同步时间。
提示：默认真实物理机已经配置为NTP服务器。
[root@node1 ~]# vim /etc/chrony.conf
… …
server 192.168.2.254   iburst
[root@node1 ~]# for i in 41  42  43
do
     scp /etc/chrony.conf 192.168.2.$i:/etc/
     ssh 192.168.2.$i "systemctl restart chronyd"
done
6）使用virt-manager为三台ceph虚拟机添加磁盘。
每台虚拟机添加3块20G的磁盘。
步骤二：部署ceph集群

1）给node1主机安装ceph-deploy，创建工作目录，初始化配置文件。
[root@node1 ~]# yum -y install ceph-deploy
[root@node1 ~]# mkdir ceph-cluster
[root@node1 ~]# cd ceph-cluster
[root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3
2）给所有ceph节点安装ceph相关软件包
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
     ssh $i "yum -y install ceph-mon ceph-osd ceph-mds"
done
[root@node1 ceph-cluster]# ceph-deploy mon create-initial
[root@node1 ceph-cluster]# ceph -s                    #查看结果
    cluster 9f3e04b8-7dbb-43da-abe6-b9e3f5e46d2e
     health HEALTH_ERR
     monmap e2: 3 mons at
 {node1=192.168.2.41:6789/0,node2=192.168.2.42:6789/0,node3=192.168.2.43:6789/0}
     
osdmap e45: 0 osds: 0 up, 0 in
3）准备磁盘分区，创建journal盘，并永久修改设备权限。
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
     ssh $i "parted /dev/vdb mklabel gpt"
     ssh $i "parted /dev/vdb mkpart primary 1 50%"
     ssh $i "parted /dev/vdb mkpart primary 50% 100%"
 done
提示：下面的步骤在所有主机都需要操作（node1，node2，node3）
#临时修改权限：
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb1
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb2
#永久修改权限：
[root@node1 ceph-cluster]# vim /etc/udev/rules.d/70-vdb.rules
ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"
4）使用ceph-deploy工具初始化数据磁盘（仅node1操作）。
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:vdc   node1:vdd    
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:vdc   node2:vdd
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:vdc   node3:vdd  
5）初始化OSD集群。
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2  
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
//一个存储设备对应一个缓存设备，缓存需要SSD，不需要很大
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2 
[root@node1 ceph-cluster]# ceph -s                 #查看集群状态
cluster 9f3e04b8-7dbb-43da-abe6-b9e3f5e46d2e
health HEALTH_OK
     monmap e2: 3 mons at {node1=192.168.4.11:6789/0,node2=192.168.4.12:6789/0,node3=192.168.4.13:6789/0}
            election epoch 6, quorum 0,1,2 node1,node2,node3
     osdmap e45: 6 osds: 6 up, 6 in
            flags sortbitwise
      pgmap v25712: 64 pgs, 1 pools, 86465 kB data, 2612 objects
            508 MB used, 119 GB / 119 GB avail
                  64 active+clean
步骤三：部署ceph文件系统

1）启动mds服务
[root@node1 ceph-cluster]# ceph-deploy mds create node3
2）创建存储池（文件系统由inode和block组成）
[root@node1 ceph-cluster]# ceph osd pool create cephfs_data 128
[root@node1 ceph-cluster]# ceph osd pool create cephfs_metadata 128
[root@node1 ceph-cluster]# ceph osd lspools
0 rbd,1 cephfs_data,2 cephfs_metadata
3）创建文件系统
[root@node1 ceph-cluster]# ceph fs new myfs1 cephfs_metadata cephfs_data
[root@node1 ceph-cluster]# ceph fs ls
name: myfs1, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
步骤四：迁移网站数据到ceph集群

1）卸载web1，web2，web3的NFS共享。
暂停服务防止有人实时读写文件。
[root@web1 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web2 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web3 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web1 ~]# umount /usr/local/nginx/html
[root@web2 ~]# umount /usr/local/nginx/html
[root@web3 ~]# umount /usr/local/nginx/html
[root@web1 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
[root@web2 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
[root@web3 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
2）web服务器永久挂载Ceph文件系统（web1、web2、web3都需要操作）。
在任意ceph节点，如node1查看ceph账户与密码。
[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
    key = AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
/etc/rc.local是开机启动脚本，任何命令放在该文件中都是开机自启。
[root@web1 ~]#  mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web1 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web1 ~]# chmod +x /etc/rc.local
[root@web2 ~]#  mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web2 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web2 ~]# chmod +x /etc/rc.local
[root@web3 ~]#  mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web3 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ \
-o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web3 ~]# chmod +x /etc/rc.local
另一种解决方案，还可以通过fstab实现永久挂载。
提示：如果希望使用fstab实现永久挂载，客户端需要额外安装libcephfs1软件包。
[root@web1 ~]# yum -y install libcephfs1
[root@web1 ~]# vim /etc/fstab
… …
192.168.4.11:/ /usr/local/nginx/html/    ceph   defaults,_netdev,name=admin,secret=AQCVcu9cWXkgKhAAWSa7qCFnFVbNCTB2DwGIOA== 0 0
3)迁移NFS服务器中的数据到Ceph存储
登陆NFS服务器备份数据，将备份数据拷贝给web1或web2或web3，tar备份数据时注意使用-f选项保留文件权限。
[root@nfs ~]# cd /web_share/html/
[root@nfs html]# tar -czpf /root/html.tar.gz ./*
[root@nfs html]# scp /root/html.tar.gz 192.168.2.11:/usr/local/nginx/html/
登陆web1将数据恢复到Ceph共享目录
[root@web1 html]# tar -xf html.tar.gz
[root@web1 html]# rm -rf html.tar.gz
4）恢复web服务
[root@web1 ~]# /usr/local/nginx/sbin/nginx
[root@web2 ~]# /usr/local/nginx/sbin/nginx
[root@web3 ~]# /usr/local/nginx/sbin/nginx
附加知识（常见面试题）

1) 如何使用awk查看TCP连接状态？
答：ss -ant |awk '{print $1}'
netstat -ant |awk '{print $6}'
2) 有个txt文件内容如下：
http://a.domain.com/l.html
http://b.domain.com/l.html
http://c.domain.com/l.html
http://a.domain.com/2.html
http://b.domain.com/2.html
http://a.domain.com/3.html
要求：得到主机名（和域名），并统计每个网址出现的次数，并排序。可以 shell 或 C
得到的结果应该是：
答：
#！/bin/bash
awk -F"[/.]" '{print $3}' txt
awk -F"[/]" '{print $3}' txt |awk '{IP[$1]++} END {for(i in IP){print IP[i],i}}' |sort –n
3) 至少说出一种linux下实现高可用的方案名称？
答：keepalived，HeartBeat
4）简述下负载均衡与高可用的概念？
答：
LB : 多台数据平均响应客户端的多次连接请求。
HA : 主备模式，主服务器宕机后 备用服务器才接替工作。












