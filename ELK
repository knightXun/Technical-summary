
日志文件的删除,备份要分析日志的重要性
ELK分别代表
Elasticsearch: 负责日志检索和存储         (核心数据,重要性排第一)
Logstash:      负责日志的收集和分析、处理  (排第二)
Kibana:        负责日志的可视化           (第三)

ELK类似于:LNMP
          web        db                  脚本语言(php,python)
          kibana     Elasticsearch       Logstash(java)       
问题1: 一个网站平时100M带宽流量,突然暴涨200M.故障排查?
   答: 1.查看出口流量看是哪个集群----->2.查看是否被攻击,查看web单ip访问量---->3.查看访问次数是否明显增加--
      -->4.CDN(内容分发网络)是否失效---->5.查看单个web的流量--->6.排查每一个链接流量(首页-->原因:首页有一个图片变大了,图片没压缩)

ELK能做什么
ELK组件在海量日志系统的运维中,可用于解决
-分布式日志数据集中式查询和管理
-系统监控,包含系统硬件和应用各个组件的监控
-故障排查
-安全信息和事件管理
-报表功能

备份3手准备:线上实时,离线,每天磁带


非关系型数据库保存:key:value(一个文档一个内容)
Node:     装有一个ES服务服务器的节点
Cluster:  有多个Node组成的集群
Document: 像行
        :库
        :表
        :列
        :索引
        :索引的拷贝
安装ES集群

vim /etc/hosts                                                      设置IP和主机对应
192.168.1.51 es1
192.168.1.52 es2
192.168.1.53 es3
192.168.1.54 es4
192.168.1.55 es5


vim /etc/yum.repos.d/local.repo                                      配置yum源

yum -y install java-1.8.0-openjdk.x86_64                            java写的,解决依赖关系
yum -y install elasticsearch
vim /etc/elasticsearch/elasticsearch.yml
17 cluster.name: myelk                                               配置集群名字
23 node.name: es1                                                    当前主机名称
54 network.host: 0.0.0.0                                             0.0.0.0（监听所有地址）
68 discovery.zen.ping.unicast.hosts: ["es1", "es2", "es3"]           声明集群里的主机成员有谁，不需要全部写进去

systemctl restart elasticsearch  systemctl enable elasticsearch      起服务并设置开机自启动

ss -antup | grep 9200 9300                                           端口号9200,9300


firefox http://192.168.1.55:9200/                                    测试
firefox http://192.168.1.55:9200/_cluster/health?pretty              测试

rpm -ql elasticsearch                                                在其中一台机器上部署插件,查看ES安装的插件路径
cd /usr/share/elasticsearch/bin                                      进去查看内容
./plugin  install 
ftp://192.168.1.254/elk/elasticsearch-head-master.zip                安装head插件
ftp://192.168.1.254/elk/elasticsearch-kopf-master.zip                安装kopf插件
ftp://192.168.1.254/elk/bigdesk-master.zip                           安装bigdesk插件

./plugin  list                                                       查看安装的插件

firefox 192.168.1.55:9200/_plugin/head                               测试
firefox 192.168.1.55:9200/_plugin/kopf                               测试
firefox 192.168.1.55:9200/_plugin/bigdesk                            测试

head:索引--->新建索引--->索引名称,分片(数量小于机器数量从0开始),副本(源数据黑框 副本浅)

http请求由三部分组成:请求行,消息报头,请求正文
http请求方法:
-常用方法:GET POST HEAD
-其他方法:OPTIONS PUT DELETE TRACE和CONNECT

ES常用
-PUT     ----增
-DELETE  ----删
-POST    ----改
--GET    ----查

curl常用参数介绍:
-A 修改请求 agent  例子    curl -A  'windows IE 11.0' http://118.144.89.240/info.php                
-X 设置请求方法     例子    curl -XPUT http://118.144.89.240/info.php                           修改请求方法
-i 显示返回头信息    

RESTful API调用
-检查集群,节点,索引的健康度,状态和统计
-管理集群,节点,索引的数据及元数据
-对索引进行CRUD操作及查询操作
-执行其他高级操作如分页,排序,过滤等

POST或PUT数据使用json格式

_cat API                                                                                     查询集群状态,节点信息
curl http://192.168.1.51:9200/_cat/indices?v   查看索引信息                                   -v参数显示详细信息                  
curl http://192.168.1.51:9200/_cat/indices?help                                             -help显示帮助信息
curl http://192.168.1.51:9200/_cat                                                           查看_cat的所有用法

例子

创建一个索引,并设置分片数量与副本数量
curl -XPUT http://es3:9200/tedu -d '
{"settings":{
    "index":{
        "number_of_hsards": 5,
        "number_of_replicas": 1
  }}
}'

firefox 192.168.1.55:9200/_plugin/head                                                      测试

PUT插入数据

curl -XPUT '3ttp://192.168.1.55:9200/tedu/student/1' -d '{
"职业":"学生",
"名字":"杰瑞",
"称号":"老鼠",
"年代":"18世纪"
}'


curl -XPUT 'http://192.168.1.55:9200/tedu/student/2' -d '{
"职业":"学生",
"名字":"大白",
"称号":"老司机",
"年代":"24世纪"
}'

*************************************************************************************************************

POST修改
curl -XPOST 'http://192.168.1.55:9200/tedu/student/1/_update' -d {
"doc":{
  "年代": "唐代"
  }
}


curl -XGET http://192.168.1.55:9200/tedu/student/1                                   查询

curl -XDELETE 'http://192.168.1.55:9200/tedu/student/1'                              删除

gunzip shakespeare.json.gz                                                           解压

curl -XPOST 'http://192.168.1.54:9200/accounts/_bulk' --data-binary @accounts.json   导入数据


***********************************************************************************************
logstach工作结构:
{数据源} ==>
       input{ } ==>
               filter { } ==>
                          output{ } ==>
                                    {ES}

安装Kibana
yum -y install kibana                                        装包
rpm -qc kibana                                               查看配置文件
vim /opt/kibana/config/kibana.yml


2 server.port: 5601                                          若把端口改为80，可以成功启动kibana，但ss时没有端口，没有监听80端口，
                                                             服务里面写死了，不能用80端口，只能是5601这个端口
5 server.host: "0.0.0.0"                                     服务器监听地址
15 elasticsearch.url: http://192.168.1.51:9200               声明地址，从哪里查，集群里面随便选一个,主机名和IP地址都可以
23 kibana.index: ".kibana"                                   kibana自己创建的索引
26 kibana.defaultAppId: "discover"                           打开kibana页面时，默认打开的页面discover
53 elasticsearch.pingTimeout: 1500                           ping检测超时时间
57 elasticsearch.requestTimeout: 30000                       请求超时
64 elasticsearch.startupTimeout: 5000                        启动超时


systemctl restart kibana 
systemctl enable  kibana

ss -antup | grep 5601                                       查看监听端口
firefox 192.168.1.56:5601                                   浏览器访问kibana
firefox http://192.168.1.55:9200/_plugin/head               用head插件访问会有.kibana的索引信息


金步国翻译英文手册,里面有很多中文翻译手册


kibana:索引名--->时间搓---->创建

/opt/logstash/bin/logstash-plugin  list          查看插件

1)安装logstash                配置主机名，ip和yum源，配置/etc/hosts（请把es1-es5、kibana主机配置和logstash一样的/etc/hosts）                              
vim /etc/hosts
192.168.1.51 es1
192.168.1.52 es2
192.168.1.53 es3
192.168.1.54 es4
192.168.1.55 es5
192.168.1.56 kibana
192.168.1.57 logstash

2)yum -y install java-1.8.0-openjdk                安装java-1.8.0-openjdk和logstash
  yum -y install logstash

touch /etc/logstash/logstash.conf                创建自定义配置文件
 vim /etc/logstash/logstash.conf
 
input{
    stdin{
   }
}
filter{
}
output{
    stdout{
   }
}
 
 /opt/logstash/bin/logstash -f /etc/logstash/logstash.conf  启动并测试
 
/opt/logstash/bin/logstash-plugin  list                     查看插件
logstash-input-stdin                                        标准输入插件
logstash-output-stdout                                      标准输出插件

/opt/logstash/bin/logstash -f /etc/logstash/logstash.conf   启动并测试

Settings: Default pipeline workers: 2
Pipeline main started
aa                                                           logstash 配置从标准输入读取输入源,然后从标准输出输出到屏幕
2018-09-15T06:19:28.724Z logstash aa

若不会写配置文件可以找帮助，插件文档的位置： https://github.com/logstash-plugins

3)codec类插件
[root@logstash ~]# vim /etc/logstash/logstash.conf
input{
    stdin{
    codec => "json"              输入设置为编码json
   }
}
filter{
}
output{
    stdout{
    codec => "rubydebug"         输出设置为rubydebug
   }
}
[root@logstash ~]#  /opt/logstash/bin/logstash -f /etc/logstash/logstash.conf 
输出的信息如下:
Settings: Default pipeline workers: 2
Pipeline main started
{"a":1}
{
             "a" => 1,
      "@version" => "1",
    "@timestamp" => "2019-03-12T03:25:58.778Z",
          "host" => "logstash"
}

4）file模块插件
[root@logstash ~]# vim /etc/logstash/logstash.conf
input{
  file {
    path          => [ "/tmp/a.log", "/tmp/b.log" ]                      创建存放信息的文件,数组,必写项                     
   sincedb_path   => "/var/lib/logstash/sincedb"                         记录读取文件的位置
   start_position => "beginning"                                         配置第一次读取文件从什么地方开始
   type           => "testlog"                                           类型名称
  }
}
filter{
}
output{
    stdout{
    codec => "rubydebug"
}
}
[root@logstash ~]# touch /tmp/a.log
[root@logstash ~]# touch /tmp/b.log
[root@logstash ~]#  /opt/logstash/bin/logstash -f  /etc/logstash/logstash.conf 


另开一个终端：写入数据
[root@logstash ~]#  echo a1 > /tmp/a.log 
[root@logstash ~]#  echo b1 > /var/tmp/b.log

之前终端查看：
 [root@logstash ~]# /opt/logstash/bin/logstash -f /etc/logstash/logstash.conf 
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => "a1",
      "@version" => "1",
    "@timestamp" => "2019-03-12T03:40:24.111Z",
          "path" => "/tmp/a.log",
          "host" => "logstash",
          "type" => "testlog"
}
{
       "message" => "b1",
      "@version" => "1",
    "@timestamp" => "2019-03-12T03:40:49.167Z",
          "path" => "/tmp/b.log",
          "host" => "logstash",
          "type" => "testlog"
}


7）filter grok插件
grok插件：
能匹配一切数据，但是性能和对资源的损耗也很大
解析各种非结构化的日志数据插件
grok使用正则表达式把飞结构化的数据结构化
在分组匹配，正则表达式需要根据具体数据结构编写
grok内置字段类型参见： https://blog.csdn.net/cui929434/article/details/94390617

解析Apache的日志
[root@es5 ~]# yum -y install httpd
[root@es5 ~]# systemctl restart httpd

浏览器访问网页，在/var/log/httpd/access_log有日志出现
[root@es5 ~]# cat /var/log/httpd/access_log
192.168.1.254 - - [12/Mar/2019:11:51:31 +0800] "GET /favicon.ico HTTP/1.1" 404 209 "-" "Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0"
[root@logstash ~]#  vim /etc/logstash/logstash.conf
input{
    file {
      path           => [ "/tmp/a.log", "/tmp/b.log" ]
      sincedb_path   => "/var/lib/logstash/sincedb"
      start_position => "beginning"
      type           => "testlog"
   }
}
filter{
    grok{
       match => [ "message",  "(?<key>reg)" ]
    }
}
output{
    stdout{
    codec => "rubydebug"
   }
}


复制/var/log/httpd/access_log的日志到logstash下的/tmp/a.log
[root@logstash ~]# vim /tmp/a.log
192.168.1.254 - - [15/Sep/2018:18:25:46 +0800] "GET / HTTP/1.1" 403 4897 "-" "Mozilla/5.0 (Windows NT 6.1; \ 
WOW64; rv:52.0) Gecko/20100101 Firefox/52.0"
[root@logstash ~]#  /opt/logstash/bin/logstash -f  /etc/logstash/logstash.conf  出现message的日志，但是没有解析是什么意思
Settings: Default pipeline workers: 2
Pipeline main started
{
       "message" => ".168.1.254 - - [15/Sep/2018:18:25:46 +0800] \"GET / HTTP/1.1\" 403 4897 \"-\" \"Mozilla/5.0  \
       (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0\"",
      "@version" => "1",
    "@timestamp" => "2018-09-15T10:26:51.335Z",
          "path" => "/tmp/a.log",
          "host" => "logstash",
          "type" => "testlog",
          "tags" => [
        [0] "_grokparsefailure"
    ]
}


若要解决没有解析的问题，同样的方法把日志复制到/tmp/a.log，logstash.conf配置文件里面修改grok
查找正则宏路径
[root@logstash ~]# cd  /opt/logstash/vendor/bundle/ \ 
jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/
[root@logstash ~]# vim grok-patterns  //查找COMBINEDAPACHELOG
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
[root@logstash ~]#  vim /etc/logstash/logstash.conf
...
filter{
   grok{
        match => ["message", "%{COMBINEDAPACHELOG}"]
  }
}
...

解析出的结果
 [root@logstash ~]#  /opt/logstash/bin/logstash -f  /etc/logstash/logstash.conf  
Settings: Default pipeline workers: 2
Pipeline main started
{
        "message" => "192.168.1.254 - - [15/Sep/2018:18:25:46 +0800] \"GET /noindex/css/open-sans.css HTTP/1.1\" 200 5081 \"http://192.168.1.65/\" \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0\"",
       "@version" => "1",
     "@timestamp" => "2018-09-15T10:55:57.743Z",
           "path" => "/tmp/a.log",
ZZ           "host" => "logstash",
           "type" => "testlog",
       "clientip" => "192.168.1.254",
          "ident" => "-",
           "auth" => "-",
      "timestamp" => "15/Sep/2019:18:25:46 +0800",
           "verb" => "GET",
        "request" => "/noindex/css/open-sans.css",
    "httpversion" => "1.1",
       "response" => "200",
          "bytes" => "5081",
       "referrer" => "\"http://192.168.1.65/\"",
          "agent" => "\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0\""
}
...

步骤二： 安装Apache服务，用filebeat收集Apache服务器的日志，并存入elasticsearch

1）在之前安装了Apache的主机上面安装filebeat
[root@se5 ~]#  yum -y install filebeat
[root@se5 ~]#  vim/etc/filebeat/filebeat.yml  
paths:
    - /var/log/httpd/access_log   //日志的路径，短横线加空格代表yml格式
document_type: apachelog    //文档类型
elasticsearch:        //加上注释
hosts: ["localhost:9200"]                //加上注释
logstash:                    //去掉注释
hosts: ["192.168.1.57:5044"]     //去掉注释,logstash那台主机的ip
[root@se5 ~]# systemctl start filebeat
[root@logstash ~]#  vim /etc/logstash/logstash.conf
input{
        stdin{ codec => "json" }
        beats{
            port => 5044
}
  file {
    path          => [ "/tmp/a.log", "/tmp/b.log" ]
   sincedb_path   => "/var/lib/logstash/sincedb"
   start_position => "beginning"
   type           => "testlog"
  }

filter{
if [type] == "apachelog"{
   grok{
        match => ["message", "%{COMBINEDAPACHELOG}"]
  }}
}
output{
      stdout{ codec => "rubydebug" }
      if [type] == "filelog"{
      elasticsearch {
          hosts => ["192.168.1.51:9200", "192.168.1.52:9200"]
          index => "filelog"
          flush_size => 2000
          idle_flush_time => 10
      }}
}
 [root@logstash logstash]#  /opt/logstash/bin/logstash  \ 
-f  /etc/logstash/logstash.conf
打开另一终端查看5044是否成功启动
[root@logstash ~]#  netstat -antup | grep 5044
tcp6       0      0 :::5044                 :::*                    LISTEN      23776/java
[root@se5 ~]#  firefox 192.168.1.55   //ip为安装filebeat的那台机器






logstash是一个数据采集,加工处理以及输出的工具
例子:
input{}     写入 
filter{}    处理
output{}    输出
codec{}     编码
--stdin    0:键盘.鼠标     标准输入
--stdout   1:显示器        标准输出
--stderr   2:             

插件
input file 
filter grok

https://www.elastic.co/guide/en/logstash/current/index.html


输入json 输出:rubydebug(输出的信息易于人们阅读)

input{
  stdin{ codec => "json" }
}
filter{}
output{
  stdout{ codec => "rubydebug" }
}

输出的信息如下:
 ./logstash -f /etc/logstash/logstash.conf        启动并验证
Settings: Default pipeline workers: 2
Pipeline main started
{"a":1,"c":3,"b":2}
{
             "a" => 1,
             "c" => 3,
             "b" => 2,
      "@version" => "1",
    "@timestamp" => "2019-08-13T06:48:33.752Z",
          "host" => "logstash"
}
^CSIGINT received. Shutting down the agent. {:level=>:warn}
stopping pipeline {:id=>"main"}

https://www.elastic.co/guide/en/logstash/current/index.html
查看官方手册:
input {
  file {
    id => "my_plugin_id"   
  }
}


https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html#plugins-inputs-file-options

找第三个是yes的

Example:

  path => [ "/var/log/messages", "/var/log/*.log" ]
  uris => [ "http://elastic.co", "http://example.net" ]

rm -rf /root/.sincedb_e9a1772295a869da80134b5c4e75816e                    存放记录日志的信息,为了后期管理一般删除掉
rm -rf /var/lib/logstash/sincedb-access
input {
  file {
    path => ["/tmp/a.log", "/var/tmp/b.log"]                              创建存放信息的文件,数组,必写项
    sincedb_path => "/var/lib/logstash/sincedb-access"                    记录读取文件的位置,这个文件路径会自动生成
    start_position => "beginning"                                         配置第一次读取文件从什么地方开始
    type => "testlog"                                                     
  }
}
filter{}
output{
  stdout{ codec => "rubydebug" } 
}





input{
  file {
    path => ["/tmp/a.log"]                                                   建存放信息的文件,数组,必写项
    sincedb_path => "/var/lib/logstash/since.db"
    start_position => "beginning"
    type => "testlog"
  }
  beats {
    port => 5044
  }
}

filter{
  if [type] == "apache_log" {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }}
}

output{
  #stdout{ codec => "rubydebug" }
  if [type] == "apache_log" {
  elasticsearch {
    hosts => ["es1:9200", "es3:9200", "es4:9200"]
    index => "weblog"
    flush_size => 2000
    idle_flush_time => 10
  }}
}












